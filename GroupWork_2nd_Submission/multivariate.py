# -*- coding: utf-8 -*-
"""“Multivariate.ipynb”的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GiwkFrenfXgsZ5n3QAUVdaWO2jvrVztP
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd 

import warnings
warnings.filterwarnings('ignore')

# %matplotlib inline

import pandas as pd
import pandas_datareader.data as web
import numpy as np

import matplotlib.pyplot as plt
import matplotlib.transforms as mtransforms
import seaborn as sns

from statsmodels.tsa.api import VARMAX
from statsmodels.tsa.stattools import acf, q_stat, adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from sklearn.preprocessing import minmax_scale
from scipy.stats import probplot, moment
from sklearn.metrics import mean_absolute_error, mean_squared_error

sns.set(style='whitegrid',
        context='notebook',
        color_codes=True)

# Correlogram Plot
def plot_correlogram(x, lags=None, title=None):    
    lags = min(10, int(len(x)/5)) if lags is None else lags
    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 8))
    x.plot(ax=axes[0][0], title='Time Series')
    x.rolling(21).mean().plot(ax=axes[0][0], c='k', lw=1)
    q_p = np.max(q_stat(acf(x, nlags=lags), len(x))[1])
    stats = f'Q-Stat: {np.max(q_p):>8.2f}\nADF: {adfuller(x)[1]:>11.2f}'
    axes[0][0].text(x=.02, y=.85, s=stats, transform=axes[0][0].transAxes)
    probplot(x, plot=axes[0][1])
    mean, var, skew, kurtosis = moment(x, moment=[1, 2, 3, 4])
    s = f'Mean: {mean:>12.2f}\nSD: {np.sqrt(var):>16.2f}\nSkew: {skew:12.2f}\nKurtosis:{kurtosis:9.2f}'
    axes[0][1].text(x=.02, y=.75, s=s, transform=axes[0][1].transAxes)
    plot_acf(x=x, lags=lags, zero=False, ax=axes[1][0])
    plot_pacf(x, lags=lags, zero=False, ax=axes[1][1])
    axes[1][0].set_xlabel('Lag')
    axes[1][1].set_xlabel('Lag')
    fig.suptitle(title, fontsize=14)
    sns.despine()
    fig.tight_layout()
    fig.subplots_adjust(top=.9)

# Unit Root Test
def test_unit_root(df):
    return df.apply(lambda x: f'{pd.Series(adfuller(x)).iloc[1]:.2%}').to_frame('p-value')

# Plot the predict value
def plot_predict(predict, df_tran, begin=-50):
  
  fig, axes = plt.subplots(nrows=5, figsize=(14, 8), sharex=True)

  predict.beer[begin:].plot(label='predicted', ax=axes[0], title='beer')
  df_tran.beer[begin:].plot(ax=axes[0], label='actual')
  axes[0].legend()

  predict.mon[begin:].plot(label='predicted', ax=axes[1], title='mon')
  df_tran.mon[begin:].plot(ax=axes[1], label='actual')
  axes[1].legend()

  predict.res[begin:].plot(label='predicted', ax=axes[2], title='res')
  df_tran.res[begin:].plot(ax=axes[2], label='actual')
  axes[2].legend()

  predict.tnt[begin:].plot(label='predicted', ax=axes[3], title='tnt')
  df_tran.tnt[begin:].plot(ax=axes[3], label='actual')
  axes[3].legend()

  predict.tot[begin:].plot(label='predicted', ax=axes[4], title='tot')
  df_tran.tot[begin:].plot(ax=axes[4], label='actual')
  axes[4].legend()

# evaluate the forecate
def forecast_evaluate(forecate, actual):
  #Calculate mean absolute error
  mae = mean_absolute_error(forecate,actual)
  #print('MAE: %f' % mae)
  #Calculate mean squared error and root mean squared error
  mse = mean_squared_error(forecate,actual)
  #print('MSE: %f' % mse)
  rmse = np.sqrt(mse)
  #print('RMSE: %f' % rmse)
  return mae, mse, rmse

# Download data and test unit root
data = pd.read_csv('Data.csv')
data = data.drop(['Date'], axis=1)
date_range = pd.date_range(start="1/1997", end="07/2020", freq='M')
data.index = date_range
data = data[['tot','tnt','mon','res','beer']]
test_unit_root(data)

plot_correlogram(data.mon)

# transform the data and make all the series are stationary
import numpy as np 
df_tran = pd.DataFrame(
    {'tnt': data.tnt.diff(1),
      'mon': np.log(data.mon).diff(1).diff(12),
      'res': np.log(data.res).diff(1).diff(12),
      'beer': data.beer.diff(1)
      }
).dropna()
test_unit_root(df_tran)
df_tran['tot'] = data['tot']['1998-02-28':]
test_unit_root(df_tran)

df_tran.plot(
    subplots=True,
    title=['tnt','mon','res','beer','tot'],
    legend=False,
    rot=0,
    figsize=(15,8)
)
sns.despine()
plt.tight_layout()

# VAR
from statsmodels.tsa.api import VAR
model_var = VAR(df_tran)

model = model_var.select_order(13)
model.summary()

# the suggested order is 1
var_results = model_var.fit(maxlags=1, ic='aic')
var_results.summary()

var_results.plot_forecast(24)

# Plot Diagnostics
model.plot_diagnostics(variable=4, figsize=(14,8), lags=24)
plt.gcf().suptitle('Beer - Diagnostics', fontsize=14)
plt.tight_layout()
plt.subplots_adjust(top=.93);

# Impulse Response Analysis
irf = var_results.irf(13)
irf.plot(orth=False)

# Forecast Error Variance Decomposition
var_results.fevd(13).plot()

# VARMAX
var_model = VARMAX(df_tran, order=(1,0), trend='c').fit(maxiter=1000)
var_model.summary()

model.plot_diagnostics(variable=3, figsize=(14,8), lags=24)
plt.gcf().suptitle('Beer - Diagnostics', fontsize=14)
plt.tight_layout()
plt.subplots_adjust(top=.93);

# Impulse-Response Function
changes = df_tran.diff().quantile(.05).tolist()  # quantile=0.01 means negative change
# print (changes)
model.impulse_responses(steps=6,
                        impulse=changes).plot.bar(subplots=True,
                                                  figsize=(12, 6),
                                                  rot=0,
                                                  legend=False)
sns.despine()
plt.tight_layout()

# Prediction
pred_steps = 12
length_of_data =len(df_tran)
start = length_of_data - pred_steps

preds = model.predict(start=start+1, end=length_of_data)
var_predict = df_tran[:start]
var_predict = pd.concat([var_predict, preds])
var_predict.index = df_tran.index

plot_predict(var_predict, df_tran)
forecast_evaluate(var_predict, df_tran)
(var_predict - df_tran).mean()



"""# VECM orginal data"""

from statsmodels.tsa.vector_ar import vecm

steps = 12
train = data[:-steps]
test = data[-steps:]

vec_rank = vecm.select_coint_rank(train, det_order = 1, k_ar_diff = 1, method = 'trace', signif=0.01)
print(vec_rank.summary())

from statsmodels.tsa.api import VECM
vecm_model = VECM(endog = train, k_ar_diff = 1, coint_rank = 1, deterministic = 'ci')
vecm_fit = vecm_model.fit()
preds = vecm_fit.predict(steps=steps)

vecm_predict = data.copy()
start = len(train)
for i in range(steps):
  for j in range(len(test.columns)):
    vecm_predict.iloc[start+i,j] = preds[i][j]

plot_predict(vecm_predict, data)
forecast_evaluate(vecm_predict, data)
(vecm_predict - data).mean()



"""# VECM Transformed Data"""

from statsmodels.tsa.vector_ar import vecm

steps = 12
train = df_tran[:-steps]
test = df_tran[-steps:]

vec_rank = vecm.select_coint_rank(train, det_order = 1, k_ar_diff = 1, method = 'trace', signif=0.01)
print(vec_rank.summary())

from statsmodels.tsa.api import VECM
vecm_model = VECM(endog = train, k_ar_diff = 1, coint_rank = 1, deterministic = 'ci')
vecm_fit = vecm_model.fit()
preds = vecm_fit.predict(steps=steps)

vecm_predict = df_tran.copy()
start = len(train)
for i in range(steps):
  for j in range(len(test.columns)):
    vecm_predict.iloc[start+i,j] = preds[i][j]

plot_predict(vecm_predict, df_tran)

print("The Sum of Squred Error of VAR:\n",evaluate(var_predict, df_tran))
print()
print("The Sum of Squred Error of VECM:\n",evaluate(vecm_predict, df_tran))

VAR (0.007245840224894107, 0.00789804717488999, 0.0888709579946677)
     (0.019977046755853538 0.05916016640809287 0.24322862991040523)

(predict - df_tran).mean()

def evaluate(predict, actual):
  compare = (predict - actual)
  return (compare**2).sum(axis=0)

mean_squared_error